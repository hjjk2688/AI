# Keras 모델 학습 심화: 출력층 활성화 함수

이 문서는 Keras 모델을 학습시킬 때, 학습 과정을 제어하는 **출력층 활성화 함수**에 대해 설명합니다.

## 1. 출력층 활성화 함수 선택하기: Linear vs. Sigmoid/Softmax

출력층의 활성화 함수는 **"내가 풀고 싶은 문제가 무엇인가?"**에 따라 결정됩니다. '정답'이 있는 것이 아니라, 문제의 종류에 따라 도구를 선택하는 것과 같습니다.

### 선형 함수(Linear)를 사용하는 경우: 회귀 (Regression)

출력층에 활성화 함수를 쓰지 않는 것(선형 함수)은 계산된 값이 **'있는 그대로'** 나오게 합니다. 이것이 필요한 경우는 바로 **회귀(Regression)** 문제입니다. 회귀란, **연속적인 숫자 값**을 예측하는 문제입니다.

#### 예시: 왜 '있는 그대로의 값'이 필요한가?

*   **집값 예측:** 모델이 예측해야 할 정답은 '5억 3천만원', '12억 1천만원' 같은 특정 숫자입니다. 이 값은 0과 1 사이에 있지도 않고, 음수가 될 수도 있습니다(예: 수익률 예측). 만약 `sigmoid`를 쓰면 출력값이 0과 1 사이로 고정되어 버려서 5억이라는 값을 절대 예측할 수 없습니다. 따라서 모델이 어떤 숫자든 자유롭게 예측할 수 있도록 '있는 그대로'의 값을 내보내는 선형 함수가 필요합니다.
*   **주가 예측:** 내일의 주가 '175,000원'을 예측해야 합니다.
*   **온도 예측:** 내일의 온도 '25.5도' 또는 '-10도'를 예측해야 합니다.

> 이처럼 예측해야 할 **정답 자체가 특정 범위에 갇혀있지 않은 실제 숫자**일 때, 출력층에 선형 함수를 사용합니다.

### Sigmoid / Softmax를 사용하는 경우: 분류 (Classification)

반대로, **분류(Classification)** 문제는 모델의 출력이 **'확률'**로 해석되어야 합니다. 이럴 때 활성화 함수를 사용해 값을 적절히 변환해줍니다.

*   **이진 분류 (Binary Classification) - `sigmoid` 사용:**
    *   **문제:** 이 메일은 '스팸'인가, '정상'인가? (둘 중 하나 맞추기)
    *   **필요한 출력:** '스팸일 확률'을 나타내는 0과 1 사이의 값 하나. (예: 0.95 -> 95% 확률로 스팸)
    *   **해결:** `sigmoid` 함수는 어떤 값이 들어오든 그 결과를 0과 1 사이로 바꿔주므로, 이런 문제에 완벽하게 들어맞습니다.

*   **다중 클래스 분류 (Multi-class Classification) - `softmax` 사용:**
    *   **문제:** 이 이미지는 '개', '고양이', '새' 중 무엇인가? (여러 개 중 하나 맞추기)
    *   **필요한 출력:** 각 클래스에 대한 확률 분포. 중요한 것은 **모든 확률의 총합이 1**이 되어야 한다는 것입니다. (예: `[개=0.1, 고양이=0.8, 새=0.1]`)
    *   **해결:** `softmax` 함수는 여러 개의 출력값을 받아서, 총합이 1이 되는 확률 분포로 변환해주는 역할을 합니다.

### 결론 요약

| 문제 유형 | 예측 대상 | 출력층 활성화 함수 | 왜? (Why?) |
| :--- | :--- | :--- | :--- |
| **회귀 (Regression)** | 연속적인 숫자 (집값, 주가 등) | **Linear (선형 함수)** | 예측값이 특정 범위(0~1)에 갇히지 않아야 함 |
| **이진 분류 (Binary)** | 둘 중 하나일 확률 | **Sigmoid (시그모이드)** | 출력을 0과 1 사이의 확률 값으로 만들어야 함 |
| **다중 분류 (Multi-class)** | 여러 개일 확률 분포 | **Softmax (소프트맥스)** | 모든 출력의 합이 1이 되는 확률 분포를 만들어야 함 |


---

## 2. 출력층 활성화 함수와 학습 속도의 관계

출력층의 활성화 함수 선택은 모델의 학습 속도와 직결됩니다. `linear`와 `sigmoid`의 차이를 통해 이를 이해할 수 있습니다.

### 2.1. `activation='linear'` vs `activation=None`

결론적으로 **두 코드는 완전히 동일하게 동작합니다.**

Keras의 `Dense` 층에서 `activation` 파라미터를 지정하지 않으면, 기본값으로 `None`이 적용됩니다. 이는 아무런 활성화 함수를 적용하지 않는다는 뜻이며, 계산된 결과(`z = w*x + b`)를 그대로 내보내는 `activation='linear'`와 같은 의미입니다.

코드를 읽는 사람이 "실수가 아니라 의도적으로 선형 출력을 사용했다"는 것을 명확히 알 수 있도록 `activation='linear'`라고 명시해주는 것이 좋은 코딩 스타일입니다.

### 2.2. 왜 `linear`가 `sigmoid`보다 훨씬 빨리 수렴하는가?

이는 두 활성화 함수가 만드는 **'손실 지형(Loss Landscape)'**의 모양이 완전히 다르기 때문입니다.

#### `activation='linear'`의 경우: 매끄러운 고속도로

*   **손실 지형:** 출력층이 선형 함수이면, 최종 손실 함수는 보통 매끄러운 그릇 모양의 2차 함수 형태(convex)가 됩니다. 이 지형에는 '지역 최적점(local minimum)'이 거의 없고, 가장 낮은 지점으로 가는 길이 명확합니다.
*   **학습 과정:** 경사 하강법이 마치 **매끄러운 고속도로**를 따라 목적지(최저점)까지 빠르게 달려가는 것과 같습니다. 손실이 빠르게 최소치에 수렴하므로, `EarlyStopping`이 학습을 일찍 중단시킵니다.

#### `activation='sigmoid'`의 경우: 늪지대가 있는 비포장도로

*   **손실 지형:** Sigmoid 함수는 양 끝이 평평한 모양을 가집니다. 만약 학습 초기에 가중치가 우연히 이 평평한 구간에 해당하는 값을 만들면, **기울기(gradient)가 0에 매우 가까워지는 '기울기 소실(Vanishing Gradient)' 현상**이 발생합니다.
*   **학습 과정:** 이것은 마치 자동차가 **깊은 늪지대**에 빠진 것과 같습니다. 바퀴가 헛돌기만 할 뿐(기울기가 0에 가까움), 앞으로 거의 나아가지 못합니다(학습이 거의 안 됨). 손실 값이 아주 조금씩만 줄어들어 `EarlyStopping` 콜백이 개선이 계속되고 있다고 착각하게 만듭니다.
*   **결과:** 결국 늪지대를 아주 느리게 빠져나오거나, 혹은 계속 헛돌면서 최대 `epochs`를 거의 다 채우게 됩니다.

**결론:** `linear`를 사용했을 때 학습이 빨리 끝난 것은 **학습이 더 효율적으로 잘 되어서** 목표에 일찍 도달했기 때문입니다. 반면, `sigmoid`를 사용했을 때는 **기울기 소실 문제(늪지대)에 빠져 학습이 매우 비효율적으로 진행되었기 때문에** 목표에 도달하지 못하고 시간만 오래 걸린 것입니다.
